{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p0MI_5H_ClkK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ljQ4AdrDbU0",
    "outputId": "bc271c29-b403-4c08-fc69-2fe7d2d56572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading necessary NLTK data...\n",
      "NLTK data download complete.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    print(\"Downloading necessary NLTK data...\")\n",
    "    nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'omw-1.4', 'averaged_perceptron_tagger_eng'], quiet=True)\n",
    "    print(\"NLTK data download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QimLovvgDc_Z"
   },
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rsWYv9LlE6O0"
   },
   "outputs": [],
   "source": [
    "def load_data(data_path='../data/twitter_training.csv', text_column='Tweet'):\n",
    "    \"\"\"Loads the dataset and prepares the DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(data_path, header=None, encoding='latin1')\n",
    "        df.columns = ['ID', 'Entity', 'Sentiment', text_column]\n",
    "        df = df.dropna(subset=[text_column]).reset_index(drop=True)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {data_path}.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "20SN9WI8E8H9"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map NLTK POS tags to WordNet POS tags\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "q4DHccCHE78k"
   },
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    hashtags = re.findall(r'#(\\w+)', text)\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return ' '.join(hashtags), ' '.join(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "upX4mpXwE724"
   },
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    tokens = tweet_tokenizer.tokenize(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-yPwGkWmGju3"
   },
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatizes tokens using POS tagging for context-aware normalization.\"\"\"\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in tagged_tokens:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if isinstance(word, str):\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wntag)\n",
    "            lemmatized_tokens.append(lemma)\n",
    "\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dJG7m3LDGjbo"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df, text_column='Tweet'):\n",
    "    df[text_column] = df[text_column].fillna('')\n",
    "\n",
    "    df['hashtags'], df['mentions'] = zip(*df[text_column].apply(extract_features))\n",
    "\n",
    "    df['tokens'] = df[text_column].apply(clean_and_tokenize)\n",
    "\n",
    "    df['lemmas'] = df['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "    df['processed_text'] = df['lemmas'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7WtR7a5jFN6U"
   },
   "outputs": [],
   "source": [
    "def vectorize_data(df, text_column='processed_text'):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df[text_column])\n",
    "\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "    return tfidf_df, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52eT8vtjGtBa",
    "outputId": "8e70d1a4-3253-458d-d916-9084702e7269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Data ---\n",
      "Successfully loaded 73996 rows. Processing a sample of 100 rows.\n",
      "\n",
      "--- Original Data Sample (First 5 rows) ---\n",
      "| Tweet                                                     |\n",
      "|:----------------------------------------------------------|\n",
      "| im getting on borderlands and i will murder you all ,     |\n",
      "| I am coming to the borders and I will kill you all,       |\n",
      "| im getting on borderlands and i will kill you all,        |\n",
      "| im coming on borderlands and i will murder you all,       |\n",
      "| im getting on borderlands 2 and i will murder you me all, |\n",
      "\n",
      "--- 2. Applying Preprocessing Pipeline ---\n",
      "\n",
      "--- 3. Processed Data Sample (First 5 rows) ---\n",
      "| Tweet                                                     | hashtags   | mentions   | processed_text                                      |\n",
      "|:----------------------------------------------------------|:-----------|:-----------|:----------------------------------------------------|\n",
      "| im getting on borderlands and i will murder you all ,     |            |            | im get on borderland and i will murder you all      |\n",
      "| I am coming to the borders and I will kill you all,       |            |            | I be come to the border and I will kill you all     |\n",
      "| im getting on borderlands and i will kill you all,        |            |            | im get on borderland and i will kill you all        |\n",
      "| im coming on borderlands and i will murder you all,       |            |            | im come on borderland and i will murder you all     |\n",
      "| im getting on borderlands 2 and i will murder you me all, |            |            | im get on borderland 2 and i will murder you me all |\n",
      "\n",
      "--- 4. Applying TF-IDF Vectorization (sublinear_tf=True) ---\n",
      "\n",
      "Total features (vocabulary size): 370\n",
      "\n",
      "--- 5. TF-IDF Vectorization Sample (First 5 rows, first 10 features) ---\n",
      "|       |   10 |   20 |   2010 |   2011 |   2wmmip5 |   45 |   5wf9jg |   610 |   ability |   about |\n",
      "|:------|-----:|-----:|-------:|-------:|----------:|-----:|---------:|------:|----------:|--------:|\n",
      "| Doc 1 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
      "| Doc 2 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
      "| Doc 3 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
      "| Doc 4 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
      "| Doc 5 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
      "\n",
      "--- Script Execution Complete ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    TEXT_COLUMN = 'Tweet'\n",
    "\n",
    "    print(\"--- 1. Loading Data ---\")\n",
    "    df = load_data(text_column=TEXT_COLUMN)\n",
    "\n",
    "    if df is None:\n",
    "        print(\"Exiting script due to data loading error.\")\n",
    "        exit()\n",
    "\n",
    "    df_sample = df.head(100).copy()\n",
    "\n",
    "    print(f\"Successfully loaded {len(df)} rows. Processing a sample of {len(df_sample)} rows.\")\n",
    "    print(\"\\n--- Original Data Sample (First 5 rows) ---\")\n",
    "    print(df_sample[[TEXT_COLUMN]].head().to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- 2. Applying Preprocessing Pipeline ---\")\n",
    "    processed_df = preprocess_data(df_sample, text_column=TEXT_COLUMN)\n",
    "\n",
    "    print(\"\\n--- 3. Processed Data Sample (First 5 rows) ---\")\n",
    "    print(processed_df[[TEXT_COLUMN, 'hashtags', 'mentions', 'processed_text']].head().to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- 4. Applying TF-IDF Vectorization (sublinear_tf=True) ---\")\n",
    "    tfidf_df, vectorizer = vectorize_data(processed_df)\n",
    "\n",
    "    print(f\"\\nTotal features (vocabulary size): {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "    print(\"\\n--- 5. TF-IDF Vectorization Sample (First 5 rows, first 10 features) ---\")\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()[:10]\n",
    "    tfidf_sample_output = tfidf_df.iloc[:5, :10]\n",
    "    tfidf_sample_output.columns = feature_names\n",
    "    tfidf_sample_output.index = [f\"Doc {i+1}\" for i in range(5)]\n",
    "\n",
    "    print(tfidf_sample_output.to_markdown())\n",
    "\n",
    "    print(\"\\n--- Script Execution Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
