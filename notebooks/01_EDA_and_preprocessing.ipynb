{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p0MI_5H_ClkK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ljQ4AdrDbU0",
    "outputId": "bc271c29-b403-4c08-fc69-2fe7d2d56572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading necessary NLTK data...\n",
      "NLTK data download complete.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    print(\"Downloading necessary NLTK data...\")\n",
    "    nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'omw-1.4', 'averaged_perceptron_tagger_eng'], quiet=True)\n",
    "    print(\"NLTK data download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QimLovvgDc_Z"
   },
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rsWYv9LlE6O0"
   },
   "outputs": [],
   "source": [
    "def load_data(data_path='twitter_training.csv', text_column='Tweet'):\n",
    "    \"\"\"Loads the dataset and prepares the DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(data_path, header=None, encoding='latin1')\n",
    "        df.columns = ['ID', 'Entity', 'Sentiment', text_column]\n",
    "        df = df.dropna(subset=[text_column]).reset_index(drop=True)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {data_path}.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "20SN9WI8E8H9"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map NLTK POS tags to WordNet POS tags\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "q4DHccCHE78k"
   },
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    hashtags = re.findall(r'#(\\w+)', text)\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return ' '.join(hashtags), ' '.join(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "upX4mpXwE724"
   },
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    tokens = tweet_tokenizer.tokenize(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-yPwGkWmGju3"
   },
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatizes tokens using POS tagging for context-aware normalization.\"\"\"\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in tagged_tokens:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if isinstance(word, str):\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wntag)\n",
    "            lemmatized_tokens.append(lemma)\n",
    "\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dJG7m3LDGjbo"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df, text_column='Tweet'):\n",
    "    df[text_column] = df[text_column].fillna('')\n",
    "\n",
    "    df['hashtags'], df['mentions'] = zip(*df[text_column].apply(extract_features))\n",
    "\n",
    "    df['tokens'] = df[text_column].apply(clean_and_tokenize)\n",
    "\n",
    "    df['lemmas'] = df['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "    df['processed_text'] = df['lemmas'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7WtR7a5jFN6U"
   },
   "outputs": [],
   "source": [
    "def vectorize_data(df, text_column='processed_text'):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df[text_column])\n",
    "\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "    return tfidf_df, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52eT8vtjGtBa",
    "outputId": "8e70d1a4-3253-458d-d916-9084702e7269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Data ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m TEXT_COLUMN = \u001b[33m'\u001b[39m\u001b[33mTweet\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- 1. Loading Data ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mload_data\u001b[49m(text_column=TEXT_COLUMN)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExiting script due to data loading error.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    TEXT_COLUMN = 'Tweet'\n",
    "\n",
    "    print(\"--- 1. Loading Data ---\")\n",
    "    df = load_data(text_column=TEXT_COLUMN)\n",
    "\n",
    "    if df is None:\n",
    "        print(\"Exiting script due to data loading error.\")\n",
    "        exit()\n",
    "\n",
    "    df_sample = df.head(100).copy()\n",
    "\n",
    "    print(f\"Successfully loaded {len(df)} rows. Processing a sample of {len(df_sample)} rows.\")\n",
    "    print(\"\\n--- Original Data Sample (First 5 rows) ---\")\n",
    "    print(df_sample[[TEXT_COLUMN]].head().to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- 2. Applying Preprocessing Pipeline ---\")\n",
    "    processed_df = preprocess_data(df_sample, text_column=TEXT_COLUMN)\n",
    "\n",
    "    print(\"\\n--- 3. Processed Data Sample (First 5 rows) ---\")\n",
    "    print(processed_df[[TEXT_COLUMN, 'hashtags', 'mentions', 'processed_text']].head().to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- 4. Applying TF-IDF Vectorization (sublinear_tf=True) ---\")\n",
    "    tfidf_df, vectorizer = vectorize_data(processed_df)\n",
    "\n",
    "    print(f\"\\nTotal features (vocabulary size): {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "    print(\"\\n--- 5. TF-IDF Vectorization Sample (First 5 rows, first 10 features) ---\")\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()[:10]\n",
    "    tfidf_sample_output = tfidf_df.iloc[:5, :10]\n",
    "    tfidf_sample_output.columns = feature_names\n",
    "    tfidf_sample_output.index = [f\"Doc {i+1}\" for i in range(5)]\n",
    "\n",
    "    print(tfidf_sample_output.to_markdown())\n",
    "\n",
    "    print(\"\\n--- Script Execution Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
