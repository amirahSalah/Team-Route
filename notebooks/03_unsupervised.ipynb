{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b74317",
   "metadata": {},
   "source": [
    "# 04 - Unsupervised\n",
    "Quick prototypes for LDA, NMF and visual validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2865be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74863a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from ../scripts\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from utils import load_data, preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28471f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just testing\n",
    "unsupervisedDF = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0ff7c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Negative      22358\n",
       "Positive      20655\n",
       "Neutral       18108\n",
       "Irrelevant    12875\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just testing\n",
    "proc_df = preprocess_data(unsupervisedDF)\n",
    "proc_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b9246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[im, getting, borderlands, murder]</td>\n",
       "      <td>[im, get, borderland, murder]</td>\n",
       "      <td>im get borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[coming, borders, kill]</td>\n",
       "      <td>[come, border, kill]</td>\n",
       "      <td>come border kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[im, getting, borderlands, kill]</td>\n",
       "      <td>[im, get, borderland, kill]</td>\n",
       "      <td>im get borderland kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[im, coming, borderlands, murder]</td>\n",
       "      <td>[im, come, borderland, murder]</td>\n",
       "      <td>im come borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[im, getting, borderlands, 2, murder]</td>\n",
       "      <td>[im, get, borderland, 2, murder]</td>\n",
       "      <td>im get borderland 2 murder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID       Entity Sentiment  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "                                               Tweet hashtags mentions  \\\n",
       "0  im getting on borderlands and i will murder yo...                     \n",
       "1  I am coming to the borders and I will kill you...                     \n",
       "2  im getting on borderlands and i will kill you ...                     \n",
       "3  im coming on borderlands and i will murder you...                     \n",
       "4  im getting on borderlands 2 and i will murder ...                     \n",
       "\n",
       "                                  tokens                            lemmas  \\\n",
       "0     [im, getting, borderlands, murder]     [im, get, borderland, murder]   \n",
       "1                [coming, borders, kill]              [come, border, kill]   \n",
       "2       [im, getting, borderlands, kill]       [im, get, borderland, kill]   \n",
       "3      [im, coming, borderlands, murder]    [im, come, borderland, murder]   \n",
       "4  [im, getting, borderlands, 2, murder]  [im, get, borderland, 2, murder]   \n",
       "\n",
       "               processed_text  \n",
       "0    im get borderland murder  \n",
       "1            come border kill  \n",
       "2      im get borderland kill  \n",
       "3   im come borderland murder  \n",
       "4  im get borderland 2 murder  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5d376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73996, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e40d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=proc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8712d64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\test\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\test\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\test\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Preprocessing:   0%|          | 0/73996 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 73996/73996 [00:17<00:00, 4197.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans silhouette (sampled): 0.0126\n",
      "Cluster 0: dead, red, red dead, redemption, dead redemption, access health, time, time commission\n",
      "Cluster 1: fuck, game, let fuck, shit, fuck fuck, let, fifa, get\n",
      "Cluster 2: like, look, look like, get, game, really, feel, play\n",
      "Cluster 3: get, good, johnson, shit, one, make, new, great\n",
      "Cluster 4: love, game, love see, much, new, show, see, show love\n",
      "Cluster 5: ban, player, detail, see detail, occur, occur see, battlefield player, battlefield\n",
      "Cluster 6: game, play, cant, wait, get, cant wait, good, fix\n",
      "Cluster 7: depot, home, home depot, work, walmart, lowes, depot get, get\n",
      "Cluster 8: unk, unk unk, welcome, cool, haha, life, youtube, win\n",
      "Cluster 9: best, ever, game, best game, one best, one, play, player\n",
      "Training LDA: 15 topics ...\n",
      " -> coherence (c_v): 0.3885\n",
      "Training LDA: 16 topics ...\n",
      " -> coherence (c_v): 0.4042\n",
      "Training LDA: 17 topics ...\n",
      " -> coherence (c_v): 0.3930\n",
      "Training LDA: 18 topics ...\n",
      " -> coherence (c_v): 0.4047\n",
      "Training LDA: 19 topics ...\n",
      " -> coherence (c_v): 0.3882\n",
      "Training LDA: 20 topics ...\n",
      " -> coherence (c_v): 0.3972\n",
      "Best number of topics by coherence: 18 0.4047186625956016\n",
      "✅ Finished topic modeling and saved models. You can now explore 'tweets_with_topics.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import spacy\n",
    "\n",
    "# vectorizers & models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# persistence & viz\n",
    "import joblib\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensim_models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure NLTK downloads\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load small spacy model if available (for lemmatization)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "except:\n",
    "    nlp = None  # if missing, run: python -m spacy download en_core_web_sm\n",
    "\n",
    "########################\n",
    "# Load CSV data\n",
    "########################\n",
    "assert 'processed_text'in df.columns, \"CSV must have a 'text' column\"\n",
    "texts = df['processed_text'].astype(str).tolist()\n",
    "\n",
    "########################\n",
    "# Preprocessing\n",
    "########################\n",
    "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False)\n",
    "\n",
    "EN_STOP = set(stopwords.words('english'))\n",
    "TW_STOP = set(['rt','https','http','co'])\n",
    "STOPWORDS = EN_STOP.union(TW_STOP)\n",
    "\n",
    "url_re = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "mention_re = re.compile(r'@\\w+')\n",
    "hashtag_re = re.compile(r'#\\w+')\n",
    "emoji_re = re.compile(\"[\" \n",
    "         u\"\\U0001F600-\\U0001F64F\"\n",
    "         u\"\\U0001F300-\\U0001F5FF\"\n",
    "         u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def clean_tweet(text):\n",
    "    text = str(text)\n",
    "    text = url_re.sub(' ', text)\n",
    "    text = mention_re.sub(' ', text)\n",
    "    text = re.sub(r'#', ' #', text)\n",
    "    text = emoji_re.sub(' ', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9# ]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "def preprocess_tokens(text, lemmatize=True, nlp=nlp):\n",
    "    txt = clean_tweet(text)\n",
    "    tokens = tknzr.tokenize(txt)\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 2]\n",
    "    tokens = [t[1:] if t.startswith('#') else t for t in tokens]\n",
    "    if lemmatize and nlp:\n",
    "        doc = nlp(\" \".join(tokens))\n",
    "        tokens = [tok.lemma_ for tok in doc if tok.lemma_ not in STOPWORDS and len(tok.lemma_) > 2]\n",
    "    return tokens\n",
    "\n",
    "tokenized = [preprocess_tokens(t) for t in tqdm(texts, desc=\"Preprocessing\")]\n",
    "joined_docs = [\" \".join(toks) for toks in tokenized]\n",
    "\n",
    "########################\n",
    "# TF-IDF & KMeans\n",
    "########################\n",
    "tfidf = TfidfVectorizer(max_df=0.95, min_df=5, ngram_range=(1,2), max_features=30000)\n",
    "X_tfidf = tfidf.fit_transform(joined_docs)\n",
    "\n",
    "k = 10\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "kmeans_labels = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "try:\n",
    "    s = silhouette_score(X_tfidf, kmeans_labels, sample_size=2000, random_state=42)\n",
    "except:\n",
    "    s = silhouette_score(X_tfidf.toarray()[:2000], kmeans_labels[:2000])\n",
    "print(f\"KMeans silhouette (sampled): {s:.4f}\")\n",
    "\n",
    "def top_terms_per_cluster(kmeans_model, vectorizer, topn=12):\n",
    "    centers = kmeans_model.cluster_centers_\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top = []\n",
    "    for i, center in enumerate(centers):\n",
    "        idx = np.argsort(center)[::-1][:topn]\n",
    "        top.append([terms[j] for j in idx])\n",
    "    return top\n",
    "\n",
    "cluster_top = top_terms_per_cluster(kmeans, tfidf, topn=12)\n",
    "for i, terms in enumerate(cluster_top):\n",
    "    print(f\"Cluster {i}: {', '.join(terms[:8])}\")\n",
    "\n",
    "########################\n",
    "# CountVectorizer & LDA\n",
    "########################\n",
    "count_vect = CountVectorizer(max_df=0.95, min_df=5, ngram_range=(1,2), max_features=30000)\n",
    "X_counts = count_vect.fit_transform(joined_docs)\n",
    "\n",
    "dictionary = Dictionary(tokenized)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.95, keep_n=30000)\n",
    "corpus_gensim = [dictionary.doc2bow(doc) for doc in tokenized]\n",
    "\n",
    "lda_models = {}\n",
    "coherence_scores = {}\n",
    "for n_topics in range(15, 21):\n",
    "    print(f\"Training LDA: {n_topics} topics ...\")\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=10, learning_method='online', random_state=42, batch_size=1024)\n",
    "    lda.fit(X_counts)\n",
    "    lda_models[n_topics] = lda\n",
    "\n",
    "    topics_words = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        topn = topic.argsort()[::-1][:15]\n",
    "        topic_terms = [count_vect.get_feature_names_out()[i] for i in topn]\n",
    "        topics_words.append(topic_terms)\n",
    "\n",
    "    cm = CoherenceModel(topics=topics_words, texts=tokenized, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_scores[n_topics] = cm.get_coherence()\n",
    "    print(f\" -> coherence (c_v): {coherence_scores[n_topics]:.4f}\")\n",
    "\n",
    "best_n = max(coherence_scores, key=coherence_scores.get)\n",
    "lda = lda_models[best_n]\n",
    "print(\"Best number of topics by coherence:\", best_n, coherence_scores[best_n])\n",
    "\n",
    "########################\n",
    "# NMF\n",
    "########################\n",
    "n_nmf = 15\n",
    "nmf = NMF(n_components=n_nmf, random_state=42, init='nndsvda', max_iter=400)\n",
    "W = nmf.fit_transform(X_tfidf)\n",
    "H = nmf.components_\n",
    "\n",
    "df['kmeans_label'] = kmeans_labels\n",
    "doc_topic_lda = lda.transform(X_counts)\n",
    "df['lda_topic'] = np.argmax(doc_topic_lda, axis=1)\n",
    "df['nmf_topic'] = np.argmax(W, axis=1)\n",
    "\n",
    "# Save models\n",
    "joblib.dump({'tfidf': tfidf, 'count_vect': count_vect, 'kmeans': kmeans, 'lda': lda, 'nmf': nmf}, \"models.joblib\")\n",
    "\n",
    "print(\"✅ Finished topic modeling and saved models. You can now explore 'tweets_with_topics.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f02a5d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>kmeans_label</th>\n",
       "      <th>lda_topic</th>\n",
       "      <th>nmf_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[im, getting, borderlands, murder]</td>\n",
       "      <td>[im, get, borderland, murder]</td>\n",
       "      <td>im get borderland murder</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[coming, borders, kill]</td>\n",
       "      <td>[come, border, kill]</td>\n",
       "      <td>come border kill</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[im, getting, borderlands, kill]</td>\n",
       "      <td>[im, get, borderland, kill]</td>\n",
       "      <td>im get borderland kill</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[im, coming, borderlands, murder]</td>\n",
       "      <td>[im, come, borderland, murder]</td>\n",
       "      <td>im come borderland murder</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[im, getting, borderlands, 2, murder]</td>\n",
       "      <td>[im, get, borderland, 2, murder]</td>\n",
       "      <td>im get borderland 2 murder</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73991</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[realized, windows, partition, mac, like, 6, y...</td>\n",
       "      <td>[realize, window, partition, mac, like, 6, yea...</td>\n",
       "      <td>realize window partition mac like 6 year behin...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73992</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[realized, mac, window, partition, 6, years, b...</td>\n",
       "      <td>[realize, mac, window, partition, 6, year, beh...</td>\n",
       "      <td>realize mac window partition 6 year behind nvi...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73993</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[realized, windows, partition, mac, 6, years, ...</td>\n",
       "      <td>[realize, window, partition, mac, 6, year, beh...</td>\n",
       "      <td>realize window partition mac 6 year behind nvi...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73994</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[realized, windows, partition, mac, like, 6, y...</td>\n",
       "      <td>[realize, window, partition, mac, like, 6, yea...</td>\n",
       "      <td>realize window partition mac like 6 year behin...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73995</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[like, windows, partition, mac, like, 6, years...</td>\n",
       "      <td>[like, window, partition, mac, like, 6, year, ...</td>\n",
       "      <td>like window partition mac like 6 year behind d...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73996 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID       Entity Sentiment  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "...     ...          ...       ...   \n",
       "73991  9200       Nvidia  Positive   \n",
       "73992  9200       Nvidia  Positive   \n",
       "73993  9200       Nvidia  Positive   \n",
       "73994  9200       Nvidia  Positive   \n",
       "73995  9200       Nvidia  Positive   \n",
       "\n",
       "                                                   Tweet hashtags mentions  \\\n",
       "0      im getting on borderlands and i will murder yo...                     \n",
       "1      I am coming to the borders and I will kill you...                     \n",
       "2      im getting on borderlands and i will kill you ...                     \n",
       "3      im coming on borderlands and i will murder you...                     \n",
       "4      im getting on borderlands 2 and i will murder ...                     \n",
       "...                                                  ...      ...      ...   \n",
       "73991  Just realized that the Windows partition of my...                     \n",
       "73992  Just realized that my Mac window partition is ...                     \n",
       "73993  Just realized the windows partition of my Mac ...                     \n",
       "73994  Just realized between the windows partition of...                     \n",
       "73995  Just like the windows partition of my Mac is l...                     \n",
       "\n",
       "                                                  tokens  \\\n",
       "0                     [im, getting, borderlands, murder]   \n",
       "1                                [coming, borders, kill]   \n",
       "2                       [im, getting, borderlands, kill]   \n",
       "3                      [im, coming, borderlands, murder]   \n",
       "4                  [im, getting, borderlands, 2, murder]   \n",
       "...                                                  ...   \n",
       "73991  [realized, windows, partition, mac, like, 6, y...   \n",
       "73992  [realized, mac, window, partition, 6, years, b...   \n",
       "73993  [realized, windows, partition, mac, 6, years, ...   \n",
       "73994  [realized, windows, partition, mac, like, 6, y...   \n",
       "73995  [like, windows, partition, mac, like, 6, years...   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "0                          [im, get, borderland, murder]   \n",
       "1                                   [come, border, kill]   \n",
       "2                            [im, get, borderland, kill]   \n",
       "3                         [im, come, borderland, murder]   \n",
       "4                       [im, get, borderland, 2, murder]   \n",
       "...                                                  ...   \n",
       "73991  [realize, window, partition, mac, like, 6, yea...   \n",
       "73992  [realize, mac, window, partition, 6, year, beh...   \n",
       "73993  [realize, window, partition, mac, 6, year, beh...   \n",
       "73994  [realize, window, partition, mac, like, 6, yea...   \n",
       "73995  [like, window, partition, mac, like, 6, year, ...   \n",
       "\n",
       "                                          processed_text  kmeans_label  \\\n",
       "0                               im get borderland murder             3   \n",
       "1                                       come border kill             3   \n",
       "2                                 im get borderland kill             3   \n",
       "3                              im come borderland murder             3   \n",
       "4                             im get borderland 2 murder             3   \n",
       "...                                                  ...           ...   \n",
       "73991  realize window partition mac like 6 year behin...             2   \n",
       "73992  realize mac window partition 6 year behind nvi...             3   \n",
       "73993  realize window partition mac 6 year behind nvi...             3   \n",
       "73994  realize window partition mac like 6 year behin...             2   \n",
       "73995  like window partition mac like 6 year behind d...             2   \n",
       "\n",
       "       lda_topic  nmf_topic  \n",
       "0             17          0  \n",
       "1             17          0  \n",
       "2             12          0  \n",
       "3             12          0  \n",
       "4             17          0  \n",
       "...          ...        ...  \n",
       "73991          2          0  \n",
       "73992          2          0  \n",
       "73993          2          0  \n",
       "73994          2          3  \n",
       "73995          2          0  \n",
       "\n",
       "[73996 rows x 12 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38086c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
