{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p0MI_5H_ClkK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\sigma\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\sigma\\anaconda3\\lib\\site-packages (from scikit-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sigma\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sigma\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sigma\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ljQ4AdrDbU0",
    "outputId": "bc271c29-b403-4c08-fc69-2fe7d2d56572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading necessary NLTK data...\n",
      "NLTK data download complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] <urlopen error [Errno 11001] getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    print(\"Downloading necessary NLTK data...\")\n",
    "    nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'omw-1.4', 'averaged_perceptron_tagger_eng'], quiet=True)\n",
    "    print(\"NLTK data download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QimLovvgDc_Z"
   },
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rsWYv9LlE6O0"
   },
   "outputs": [],
   "source": [
    "def load_data(data_path='twitter_training.csv', text_column='text'):\n",
    "    \"\"\"Loads the dataset and prepares the DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(data_path, header=None, encoding='latin1')\n",
    "        df.columns = ['ID', 'Entity', 'sentiment', text_column]\n",
    "        df = df.dropna(subset=[text_column]).reset_index(drop=True)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {data_path}.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "20SN9WI8E8H9"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map NLTK POS tags to WordNet POS tags\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "q4DHccCHE78k"
   },
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    hashtags = re.findall(r'#(\\w+)', text)\n",
    "    mentions = re.findall(r'@(\\w+)', text)\n",
    "    return ' '.join(hashtags), ' '.join(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upX4mpXwE724"
   },
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    tokens = tweet_tokenizer.tokenize(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-yPwGkWmGju3"
   },
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatizes tokens using POS tagging for context-aware normalization.\"\"\"\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in tagged_tokens:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if isinstance(word, str):\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wntag)\n",
    "            lemmatized_tokens.append(lemma)\n",
    "\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dJG7m3LDGjbo"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df, text_column='text'):\n",
    "    df[text_column] = df[text_column].fillna('')\n",
    "\n",
    "    df['hashtags'], df['mentions'] = zip(*df[text_column].apply(extract_features))\n",
    "\n",
    "    df['tokens'] = df[text_column].apply(clean_and_tokenize)\n",
    "\n",
    "    df['lemmas'] = df['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "    df['processed_text'] = df['lemmas'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7WtR7a5jFN6U"
   },
   "outputs": [],
   "source": [
    "def vectorize_data(df, text_column='processed_text'):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df[text_column])\n",
    "\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "    return tfidf_df, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52eT8vtjGtBa",
    "outputId": "8e70d1a4-3253-458d-d916-9084702e7269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Data ---\n",
      "Successfully loaded 73996 rows. Processing a sample of 100 rows.\n",
      "\n",
      "--- Original Data Sample (First 5 rows) ---\n",
      "| text                                                      |\n",
      "|:----------------------------------------------------------|\n",
      "| im getting on borderlands and i will murder you all ,     |\n",
      "| I am coming to the borders and I will kill you all,       |\n",
      "| im getting on borderlands and i will kill you all,        |\n",
      "| im coming on borderlands and i will murder you all,       |\n",
      "| im getting on borderlands 2 and i will murder you me all, |\n",
      "\n",
      "--- 2. Applying Preprocessing Pipeline ---\n",
      "\n",
      "--- 3. Processed Data Sample (First 5 rows) ---\n",
      "| text                                                      | hashtags   | mentions   | processed_text                                      |\n",
      "|:----------------------------------------------------------|:-----------|:-----------|:----------------------------------------------------|\n",
      "| im getting on borderlands and i will murder you all ,     |            |            | im get on borderland and i will murder you all      |\n",
      "| I am coming to the borders and I will kill you all,       |            |            | I be come to the border and I will kill you all     |\n",
      "| im getting on borderlands and i will kill you all,        |            |            | im get on borderland and i will kill you all        |\n",
      "| im coming on borderlands and i will murder you all,       |            |            | im come on borderland and i will murder you all     |\n",
      "| im getting on borderlands 2 and i will murder you me all, |            |            | im get on borderland 2 and i will murder you me all |\n",
      "\n",
      "--- 4. Applying TF-IDF Vectorization (sublinear_tf=True) ---\n",
      "\n",
      "Total features (vocabulary size): 370\n",
      "\n",
      "--- 5. TF-IDF Vectorization Sample (First 5 rows, first 10 features) ---\n",
      "|       |   10 |   20 |   2010 |   2011 |   2wmmip5 |   45 |   5wf9jg |   610 |   ability |   about |\n",
      "|:------|-----:|-----:|-------:|-------:|----------:|-----:|---------:|------:|----------:|--------:|\n",
      "| Doc 1 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
      "| Doc 2 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
      "| Doc 3 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
      "| Doc 4 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
      "| Doc 5 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
      "\n",
      "--- Script Execution Complete ---\n"
     ]
    }
   ],
   "source": [
    "TEXT_COLUMN = 'text'\n",
    "\n",
    "print(\"--- 1. Loading Data ---\")\n",
    "df = load_data(text_column=TEXT_COLUMN)\n",
    "\n",
    "if df is None:\n",
    "    print(\"Exiting script due to data loading error.\")\n",
    "else:\n",
    "    df_sample = df.head(100).copy()\n",
    "\n",
    "    print(f\"Successfully loaded {len(df)} rows. Processing a sample of {len(df_sample)} rows.\")\n",
    "    print(\"\\n--- Original Data Sample (First 5 rows) ---\")\n",
    "    print(df_sample[[TEXT_COLUMN]].head().to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- 2. Applying Preprocessing Pipeline ---\")\n",
    "    processed_df = preprocess_data(df_sample, text_column=TEXT_COLUMN)\n",
    "\n",
    "    print(\"\\n--- 3. Processed Data Sample (First 5 rows) ---\")\n",
    "    print(processed_df[[TEXT_COLUMN, 'hashtags', 'mentions', 'processed_text']].head().to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- 4. Applying TF-IDF Vectorization (sublinear_tf=True) ---\")\n",
    "    tfidf_df, vectorizer = vectorize_data(processed_df)\n",
    "\n",
    "    print(f\"\\nTotal features (vocabulary size): {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "    print(\"\\n--- 5. TF-IDF Vectorization Sample (First 5 rows, first 10 features) ---\")\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()[:10]\n",
    "    tfidf_sample_output = tfidf_df.iloc[:5, :10]\n",
    "    tfidf_sample_output.columns = feature_names\n",
    "    tfidf_sample_output.index = [f\"Doc {i+1}\" for i in range(5)]\n",
    "\n",
    "    print(tfidf_sample_output.to_markdown())\n",
    "\n",
    "    print(\"\\n--- Script Execution Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "599hfKe-GyRh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\SIgma\\\\OneDrive\\\\Documents\\\\GitHub\\\\Team-Route\\\\notebooks'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=load_data(r\"c:\\Users\\SIgma\\OneDrive\\Documents\\GitHub\\Team-Route\\notebooks\\twitter_training.csv\")\n",
    "df=preprocess_data(df, text_column='text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am coming to the borders and i will kill you...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im getting on borderlands and i will kill you all</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting on borderlands  and i will murder y...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  im getting on borderlands and i will murder yo...  Positive\n",
       "1  i am coming to the borders and i will kill you...  Positive\n",
       "2  im getting on borderlands and i will kill you all  Positive\n",
       "3  im coming on borderlands and i will murder you...  Positive\n",
       "4  im getting on borderlands  and i will murder y...  Positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Neutral', 'Negative', 'Irrelevant'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am coming to the borders and i will kill you...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im getting on borderlands and i will kill you all</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting on borderlands  and i will murder y...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  im getting on borderlands and i will murder yo...          3\n",
       "1  i am coming to the borders and i will kill you...          3\n",
       "2  im getting on borderlands and i will kill you all          3\n",
       "3  im coming on borderlands and i will murder you...          3\n",
       "4  im getting on borderlands  and i will murder y...          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73996 entries, 0 to 73995\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       73996 non-null  object\n",
      " 1   sentiment  73996 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    22358\n",
       "3    20655\n",
       "2    18108\n",
       "0    12875\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "le=LabelEncoder()\n",
    "\n",
    "display(df['sentiment'].unique())\n",
    "df['sentiment']=le.fit_transform(df['sentiment'])\n",
    "\n",
    "display(df.head())\n",
    "df.info()\n",
    "display(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9660703539848328}]\n",
      "[{'label': 'negative', 'score': 0.8278113603591919}]\n",
      "accuracy DistilBERT (RoBERTa-twitter) المطور لـ4 فئات: 0.0000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988f6c7f8a8a43dda766b05d5bea2404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7701b67f12849c9b6e6780b7a73cdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIgma\\AppData\\Local\\Temp\\ipykernel_1528\\2921107161.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "جاري التدريب السريع... هيخلّص في أقل من دقيقة!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SIgma\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='167' max='464' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [167/464 43:35 < 1:18:27, 0.06 it/s, Epoch 0.72/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =--------- DistilBERT ---------=\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4, ignore_mismatched_sizes=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "\n",
    "train_df = df.sample(frac=0.1, random_state=42)  \n",
    "val_df = df.sample(frac=0.02, random_state=99)   \n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'sentiment']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['text', 'sentiment']])\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"sentiment\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"sentiment\", \"labels\")\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fast_model\",\n",
    "    num_train_epochs=2,                  \n",
    "    per_device_train_batch_size=32,       \n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_steps=20,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"جاري التدريب السريع... هيخلّص في أقل من دقيقة!\")\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(f\"الدقة السريعة: {results['eval_accuracy']:.4f}\")  \n",
    "\n",
    "\n",
    "trainer.save_model(\"my_fast_model\")\n",
    "tokenizer.save_pretrained(\"my_fast_model\")\n",
    "print(\"خلّص وحفظ النموذج!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False,  True],\n",
       "       [False, False, False,  True],\n",
       "       [False, False, False,  True],\n",
       "       ...,\n",
       "       [False, False, False,  True],\n",
       "       [False, False, False,  True],\n",
       "       [False, False, False,  True]], shape=(73996, 4))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1628,   12,   26],\n",
       "       [   0,    0,    0, ...,  399,   12,   26],\n",
       "       [   0,    0,    0, ...,  399,   12,   26],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  112,  278, 1979],\n",
       "       [   0,    0,    0, ..., 1865,  157, 1979],\n",
       "       [   0,    0,    0, ...,    2,  278, 1979]],\n",
       "      shape=(73996, 50), dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SIgma\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925/925 - 34s - 37ms/step - accuracy: 0.5664 - loss: 1.0334 - val_accuracy: 0.6526 - val_loss: 0.8716\n",
      "Epoch 2/5\n",
      "925/925 - 31s - 34ms/step - accuracy: 0.6980 - loss: 0.7684 - val_accuracy: 0.7082 - val_loss: 0.7513\n",
      "Epoch 3/5\n",
      "925/925 - 33s - 36ms/step - accuracy: 0.7526 - loss: 0.6431 - val_accuracy: 0.7388 - val_loss: 0.6824\n",
      "Epoch 4/5\n",
      "925/925 - 35s - 38ms/step - accuracy: 0.7861 - loss: 0.5576 - val_accuracy: 0.7611 - val_loss: 0.6427\n",
      "Epoch 5/5\n",
      "925/925 - 69s - 74ms/step - accuracy: 0.8091 - loss: 0.4996 - val_accuracy: 0.7724 - val_loss: 0.6190\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.7724 - loss: 0.6190\n",
      "دقة LSTM: 0.7724\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------------------- LSTM -----------------------------------------------\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "X = tokenizer.texts_to_sequences(df['text'])\n",
    "X = pad_sequences(X, maxlen=50) \n",
    "\n",
    "y = pd.get_dummies(df['sentiment']).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "display(y)\n",
    "display(X)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(5000, 128, input_length=50))\n",
    "model_lstm.add(LSTM(128, dropout=0.5))\n",
    "model_lstm.add(Dense(4, activation='softmax')) # ['Positive', 'Neutral', 'Negative', 'Irrelevant'],\n",
    "\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model_lstm.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "acc_lstm = model_lstm.evaluate(X_test, y_test)[1]\n",
    "print(f\"دقة LSTM: {acc_lstm:.4f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "925/925 - 13s - 14ms/step - accuracy: 0.5840 - loss: 1.0121 - val_accuracy: 0.7016 - val_loss: 0.7781\n",
      "Epoch 2/5\n",
      "925/925 - 12s - 13ms/step - accuracy: 0.7929 - loss: 0.5742 - val_accuracy: 0.7920 - val_loss: 0.5542\n",
      "Epoch 3/5\n",
      "925/925 - 10s - 11ms/step - accuracy: 0.8890 - loss: 0.3176 - val_accuracy: 0.8205 - val_loss: 0.5043\n",
      "Epoch 4/5\n",
      "925/925 - 10s - 11ms/step - accuracy: 0.9253 - loss: 0.2048 - val_accuracy: 0.8304 - val_loss: 0.5181\n",
      "Epoch 5/5\n",
      "925/925 - 9s - 10ms/step - accuracy: 0.9404 - loss: 0.1600 - val_accuracy: 0.8376 - val_loss: 0.5322\n",
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8376 - loss: 0.5322\n",
      " CNN: 0.8376\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------------------- CNN -----------------------------------------------\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(5000, 128, input_length=50))\n",
    "model_cnn.add(Conv1D(128, 5, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(64, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_cnn.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "acc_cnn = model_cnn.evaluate(X_test, y_test)[1]\n",
    "print(f\" CNN: {acc_cnn:.4f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LSTM        → 0.7724\n",
      "CNN         → 0.8376\n",
      "DistilBERT  → 0.1660\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"LSTM        → {acc_lstm:.4f}\")\n",
    "print(f\"CNN         → {acc_cnn:.4f}\")\n",
    "print(f\"DistilBERT  → {acc:.4f}\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
