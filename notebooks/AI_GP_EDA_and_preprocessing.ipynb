{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "Pkp1GZ5lIRSR",
        "outputId": "17e86ba1-6ed9-47ed-c25c-f955082d7995"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HalFEaezJXve"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcMlnPz5JX3x",
        "outputId": "75c41026-7f01-4378-fc2f-05e7e052fbd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading necessary NLTK data...\n",
            "NLTK data download complete.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    print(\"Downloading necessary NLTK data...\")\n",
        "    nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'omw-1.4', 'averaged_perceptron_tagger_eng'], quiet=True)\n",
        "    print(\"NLTK data download complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UwhodhvnIRfs"
      },
      "outputs": [],
      "source": [
        "tweet_tokenizer = TweetTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yFEdzzaiIRv0"
      },
      "outputs": [],
      "source": [
        "def load_data(data_path='../data/twitter_training.csv', text_column='Tweet'):\n",
        "    \"\"\"Loads the dataset and prepares the DataFrame.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(data_path, header=None, encoding='latin1')\n",
        "        df.columns = ['ID', 'Entity', 'Sentiment', text_column]\n",
        "        df = df.dropna(subset=[text_column]).reset_index(drop=True)\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Data file not found at {data_path}.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pz3a4RkzIR9N"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(tag):\n",
        "    \"\"\"Map NLTK POS tags to WordNet POS tags\"\"\"\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WPbfmdj9ISJv"
      },
      "outputs": [],
      "source": [
        "def extract_features(text):\n",
        "    hashtags = re.findall(r'#(\\w+)', text)\n",
        "    mentions = re.findall(r'@(\\w+)', text)\n",
        "    return ' '.join(hashtags), ' '.join(mentions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dv1aL7EPISXT"
      },
      "outputs": [],
      "source": [
        "def clean_and_tokenize(text):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
        "    tokens = tweet_tokenizer.tokenize(text)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xfGtsq-LISll"
      },
      "outputs": [],
      "source": [
        "def lemmatize_tokens(tokens):\n",
        "    \"\"\"Lemmatizes tokens using POS tagging for context-aware normalization.\"\"\"\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    lemmatized_tokens = []\n",
        "    for word, tag in tagged_tokens:\n",
        "        wntag = get_wordnet_pos(tag)\n",
        "        if isinstance(word, str):\n",
        "            lemma = lemmatizer.lemmatize(word, pos=wntag)\n",
        "            lemmatized_tokens.append(lemma)\n",
        "\n",
        "    return lemmatized_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wea-H6V8ISxm"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(df, text_column='Tweet'):\n",
        "    df[text_column] = df[text_column].fillna('')\n",
        "\n",
        "    df['hashtags'], df['mentions'] = zip(*df[text_column].apply(extract_features))\n",
        "\n",
        "    df['tokens'] = df[text_column].apply(clean_and_tokenize)\n",
        "\n",
        "    df['lemmas'] = df['tokens'].apply(lemmatize_tokens)\n",
        "\n",
        "    df['processed_text'] = df['lemmas'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cVF-gJQ1IS-o"
      },
      "outputs": [],
      "source": [
        "def vectorize_data(df, text_column='processed_text'):\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
        "\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(df[text_column])\n",
        "\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "    return tfidf_df, tfidf_vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es7xTp6FITLK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 1. Loading Data ---\n",
            "Error: Data file not found at ../data/twitter_training.csv.\n",
            "Exiting script due to data loading error.\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'head'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExiting script due to data loading error.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m     exit()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df_sample = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m(\u001b[32m100\u001b[39m).copy()\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows. Processing a sample of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_sample)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Original Data Sample (First 5 rows) ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'head'"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    TEXT_COLUMN = 'Tweet'\n",
        "\n",
        "    print(\"--- 1. Loading Data ---\")\n",
        "    df = load_data(text_column=TEXT_COLUMN)\n",
        "\n",
        "    if df is None:\n",
        "        print(\"Exiting script due to data loading error.\")\n",
        "        exit()\n",
        "\n",
        "    df_sample = df.head(100).copy()\n",
        "\n",
        "    print(f\"Successfully loaded {len(df)} rows. Processing a sample of {len(df_sample)} rows.\")\n",
        "    print(\"\\n--- Original Data Sample (First 5 rows) ---\")\n",
        "    print(df_sample[[TEXT_COLUMN]].head().to_markdown(index=False))\n",
        "\n",
        "    print(\"\\n--- 2. Applying Preprocessing Pipeline ---\")\n",
        "    processed_df = preprocess_data(df_sample, text_column=TEXT_COLUMN)\n",
        "\n",
        "    print(\"\\n--- 3. Processed Data Sample (First 5 rows) ---\")\n",
        "    print(processed_df[[TEXT_COLUMN, 'hashtags', 'mentions', 'processed_text']].head().to_markdown(index=False))\n",
        "\n",
        "    print(\"\\n--- 4. Applying TF-IDF Vectorization (sublinear_tf=True) ---\")\n",
        "    tfidf_df, vectorizer = vectorize_data(processed_df)\n",
        "\n",
        "    print(f\"\\nTotal features (vocabulary size): {len(vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "    print(\"\\n--- 5. TF-IDF Vectorization Sample (First 5 rows, first 10 features) ---\")\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()[:10]\n",
        "    tfidf_sample_output = tfidf_df.iloc[:5, :10]\n",
        "    tfidf_sample_output.columns = feature_names\n",
        "    tfidf_sample_output.index = [f\"Doc {i+1}\" for i in range(5)]\n",
        "\n",
        "    print(tfidf_sample_output.to_markdown())\n",
        "\n",
        "    print(\"\\n--- Script Execution Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "collapsed": true,
        "id": "BEEOcQHnITY6",
        "outputId": "f9911e3f-bd7b-4ad1-f57b-09b8a8882ae6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "features = ['age', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc',\n",
        "            'smoke', 'alco', 'active', 'gender', 'cardio']\n",
        "corr_matrix = df_clean[features].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix,\n",
        "            annot=True,\n",
        "            fmt=\".2f\",\n",
        "            cmap='coolwarm',\n",
        "            cbar=True,\n",
        "            square=True,\n",
        "            linewidths=.5,\n",
        "            linecolor='black')\n",
        "plt.title('Correlation Matrix of Clinical Features and Cardiovascular Disease')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCorrelation with Cardiovascular Disease (cardio):\")\n",
        "print(corr_matrix['cardio'].sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60bbbe1e",
        "outputId": "469b456e-3414-47af-c28d-f6f37a081aa3"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = '/content/cardio_train.csv.zip'\n",
        "\n",
        "extraction_dir = '/content/'\n",
        "\n",
        "os.makedirs(extraction_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_dir)\n",
        "\n",
        "print(f\"'{zip_file_path}' unzipped to '{extraction_dir}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dd85bf87",
        "outputId": "455a70a3-58ae-414b-d998-e4239c686cd3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "features = ['age', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc',\n",
        "            'smoke', 'alco', 'active', 'gender', 'cardio']\n",
        "\n",
        "# Ensure all specified features exist in the DataFrame\n",
        "missing_features = [f for f in features if f not in df_clean.columns]\n",
        "if missing_features:\n",
        "    print(f\"Warning: The following features are missing from df_clean: {missing_features}\")\n",
        "    # Filter out missing features for correlation calculation\n",
        "    available_features = [f for f in features if f in df_clean.columns]\n",
        "else:\n",
        "    available_features = features\n",
        "\n",
        "corr_matrix = df_clean[available_features].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix,\n",
        "            annot=True,\n",
        "            fmt=\".2f\",\n",
        "            cmap='coolwarm',\n",
        "            cbar=True,\n",
        "            square=True,\n",
        "            linewidths=.5,\n",
        "            linecolor='black')\n",
        "plt.title('Correlation Matrix of Clinical Features and Cardiovascular Disease')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCorrelation with Cardiovascular Disease (cardio):\")\n",
        "if 'cardio' in available_features:\n",
        "    print(corr_matrix['cardio'].sort_values(ascending=False))\n",
        "else:\n",
        "    print(\" 'cardio' feature is not available for correlation calculation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "XeQEXf6JITo-",
        "outputId": "e6e9b23d-1aaf-4fc0-c137-18c419734047"
      },
      "outputs": [],
      "source": [
        "df_eda = df_clean.copy()\n",
        "df_eda['age_years'] = (df_eda['age'] / 365.25).round().astype(int)\n",
        "\n",
        "min_age = df_eda['age_years'].min()\n",
        "max_age = df_eda['age_years'].max()\n",
        "bins = range(min_age, max_age + 6, 5)\n",
        "\n",
        "df_eda['age_bin'] = pd.cut(df_eda['age_years'],\n",
        "                           bins=bins,\n",
        "                           right=False,\n",
        "                           labels=[f'{i}-{i+4}' for i in bins[:-1]])\n",
        "\n",
        "prevalence_by_age = df_eda.groupby('age_bin', observed=False)['cardio'].agg(\n",
        "    total_patients=('size'),\n",
        "    disease_prevalence=('mean')\n",
        ").reset_index()\n",
        "\n",
        "print(\"Disease Prevalence by 5-Year Age Bin:\")\n",
        "print(prevalence_by_age)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='age_bin', y='disease_prevalence', data=prevalence_by_age, palette='viridis')\n",
        "plt.title('Cardiovascular Disease Prevalence by 5-Year Age Group')\n",
        "plt.xlabel('Age Bin (Years)')\n",
        "plt.ylabel('Disease Prevalence (Ratio)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "GDG8HBsCIT4X",
        "outputId": "489e4f35-ad4d-4b81-b7bd-0a5e2dd4d97b"
      },
      "outputs": [],
      "source": [
        "def categorize_bp(ap_hi, ap_lo):\n",
        "    if ap_hi < 120 and ap_lo < 80:\n",
        "        return 'Normal'\n",
        "    elif (ap_hi >= 140 or ap_lo >= 90):\n",
        "        return 'Stage 2 Hypertension'\n",
        "    elif (ap_hi >= 130 and ap_hi < 140) or (ap_lo >= 80 and ap_lo < 90):\n",
        "        return 'Stage 1 Hypertension'\n",
        "    elif (ap_hi >= 120 and ap_hi < 130) and (ap_lo < 80):\n",
        "        return 'Elevated/Pre-Hypertension'\n",
        "    else:\n",
        "        return 'Stage 1 Hypertension'\n",
        "\n",
        "df_eda['bp_category'] = df_eda.apply(\n",
        "    lambda row: categorize_bp(row['ap_hi'], row['ap_lo']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "bp_counts = df_eda['bp_category'].value_counts().sort_index()\n",
        "sns.barplot(x=bp_counts.index, y=bp_counts.values, palette='plasma')\n",
        "plt.title('Patient Distribution by Blood Pressure Category')\n",
        "plt.xlabel('Blood Pressure Category')\n",
        "plt.ylabel('Number of Patients')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Nmk94hV8IUFK",
        "outputId": "ac991a2a-2601-4921-e20e-7ba42b701086"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(df_eda,\n",
        "                 x='age_years',\n",
        "                 y='weight',\n",
        "                 color='cardio',\n",
        "                 color_discrete_map={0: 'blue', 1: 'red'},\n",
        "                 title='Age vs. Weight by Cardiovascular Disease Status',\n",
        "                 labels={'cardio': 'Cardio Disease (1=Yes, 0=No)'},\n",
        "                 hover_data=['ap_hi', 'ap_lo', 'cholesterol'])\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6a4fD6AIUU7",
        "outputId": "fb5f8e40-3215-4080-a04b-c93b4af3088f"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "contingency_table = pd.crosstab(df_eda['cholesterol'], df_eda['cardio'])\n",
        "\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"Chi-Square Test: Cholesterol vs. Cardiovascular Disease\")\n",
        "print(f\"Chi-square statistic: {chi2:.2f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Conclusion: The relationship between cholesterol level and cardiovascular disease is statistically significant.\")\n",
        "else:\n",
        "    print(\"Conclusion: The relationship is NOT statistically significant.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SGyZFLTIUhp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv8PHVTDIUuz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YpjQ2p7IU7y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSLcyAkrIVSR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVOTJR7mIVhh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQFuHm8hIVyO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycLkYJC6IWBy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "BrandWatch (.venv)",
      "language": "python",
      "name": "brandwatch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
