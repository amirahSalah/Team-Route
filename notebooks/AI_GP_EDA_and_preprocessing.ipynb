{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "Pkp1GZ5lIRSR",
        "outputId": "17e86ba1-6ed9-47ed-c25c-f955082d7995"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: click in /Users/amira_salah/Documents/Study/AI & ML Course/Graduation Project/Team-Route/.venv/lib/python3.13/site-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /Users/amira_salah/Documents/Study/AI & ML Course/Graduation Project/Team-Route/.venv/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Using cached regex-2025.11.3-cp313-cp313-macosx_10_13_x86_64.whl.metadata (40 kB)\n",
            "Collecting tqdm (from nltk)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
            "Using cached regex-2025.11.3-cp313-cp313-macosx_10_13_x86_64.whl (291 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, regex, nltk\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [nltk][32m2/3\u001b[0m [nltk]]\n",
            "\u001b[1A\u001b[2KSuccessfully installed nltk-3.9.2 regex-2025.11.3 tqdm-4.67.1\n"
          ]
        }
      ],
      "source": [
        "# !pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HalFEaezJXve"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcMlnPz5JX3x",
        "outputId": "75c41026-7f01-4378-fc2f-05e7e052fbd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading necessary NLTK data...\n",
            "NLTK data download complete.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    print(\"Downloading necessary NLTK data...\")\n",
        "    nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'omw-1.4', 'averaged_perceptron_tagger_eng'], quiet=True)\n",
        "    print(\"NLTK data download complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import functions from ../scripts\n",
        "import sys\n",
        "sys.path.append('../scripts')\n",
        "from utils import load_data, get_wordnet_pos, extract_features,clean_and_tokenize,lemmatize_tokens,preprocess_data, vectorize_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tabulate\n",
            "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: tabulate\n",
            "Successfully installed tabulate-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Es7xTp6FITLK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 1. Loading Data ---\n",
            "Successfully loaded 73996 rows. Processing a sample of 100 rows.\n",
            "\n",
            "--- Original Data Sample (First 5 rows) ---\n",
            "| Tweet                                                     |\n",
            "|:----------------------------------------------------------|\n",
            "| im getting on borderlands and i will murder you all ,     |\n",
            "| I am coming to the borders and I will kill you all,       |\n",
            "| im getting on borderlands and i will kill you all,        |\n",
            "| im coming on borderlands and i will murder you all,       |\n",
            "| im getting on borderlands 2 and i will murder you me all, |\n",
            "\n",
            "--- 2. Applying Preprocessing Pipeline ---\n",
            "\n",
            "--- 3. Processed Data Sample (First 5 rows) ---\n",
            "| Tweet                                                     | hashtags   | mentions   | processed_text                                      |\n",
            "|:----------------------------------------------------------|:-----------|:-----------|:----------------------------------------------------|\n",
            "| im getting on borderlands and i will murder you all ,     |            |            | im get on borderland and i will murder you all      |\n",
            "| I am coming to the borders and I will kill you all,       |            |            | I be come to the border and I will kill you all     |\n",
            "| im getting on borderlands and i will kill you all,        |            |            | im get on borderland and i will kill you all        |\n",
            "| im coming on borderlands and i will murder you all,       |            |            | im come on borderland and i will murder you all     |\n",
            "| im getting on borderlands 2 and i will murder you me all, |            |            | im get on borderland 2 and i will murder you me all |\n",
            "\n",
            "--- 4. Applying TF-IDF Vectorization (sublinear_tf=True) ---\n",
            "\n",
            "Total features (vocabulary size): 370\n",
            "\n",
            "--- 5. TF-IDF Vectorization Sample (First 5 rows, first 10 features) ---\n",
            "|       |   10 |   20 |   2010 |   2011 |   2wmmip5 |   45 |   5wf9jg |   610 |   ability |   about |\n",
            "|:------|-----:|-----:|-------:|-------:|----------:|-----:|---------:|------:|----------:|--------:|\n",
            "| Doc 1 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
            "| Doc 2 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
            "| Doc 3 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
            "| Doc 4 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
            "| Doc 5 |    0 |    0 |      0 |      0 |         0 |    0 |        0 |     0 |         0 |       0 |\n",
            "\n",
            "--- Script Execution Complete ---\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    TEXT_COLUMN = 'Tweet'\n",
        "\n",
        "    print(\"--- 1. Loading Data ---\")\n",
        "    df = load_data(text_column=TEXT_COLUMN)\n",
        "\n",
        "    if df is None:\n",
        "        print(\"Exiting script due to data loading error.\")\n",
        "        exit()\n",
        "\n",
        "    df_sample = df.head(100).copy()\n",
        "\n",
        "    print(f\"Successfully loaded {len(df)} rows. Processing a sample of {len(df_sample)} rows.\")\n",
        "    print(\"\\n--- Original Data Sample (First 5 rows) ---\")\n",
        "    print(df_sample[[TEXT_COLUMN]].head().to_markdown(index=False))\n",
        "\n",
        "    print(\"\\n--- 2. Applying Preprocessing Pipeline ---\")\n",
        "    processed_df = preprocess_data(df_sample, text_column=TEXT_COLUMN)\n",
        "\n",
        "    print(\"\\n--- 3. Processed Data Sample (First 5 rows) ---\")\n",
        "    print(processed_df[[TEXT_COLUMN, 'hashtags', 'mentions', 'processed_text']].head().to_markdown(index=False))\n",
        "\n",
        "    print(\"\\n--- 4. Applying TF-IDF Vectorization (sublinear_tf=True) ---\")\n",
        "    tfidf_df, vectorizer = vectorize_data(processed_df)\n",
        "\n",
        "    print(f\"\\nTotal features (vocabulary size): {len(vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "    print(\"\\n--- 5. TF-IDF Vectorization Sample (First 5 rows, first 10 features) ---\")\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()[:10]\n",
        "    tfidf_sample_output = tfidf_df.iloc[:5, :10]\n",
        "    tfidf_sample_output.columns = feature_names\n",
        "    tfidf_sample_output.index = [f\"Doc {i+1}\" for i in range(5)]\n",
        "\n",
        "    print(tfidf_sample_output.to_markdown())\n",
        "\n",
        "    print(\"\\n--- Script Execution Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>20</th>\n",
              "      <th>2010</th>\n",
              "      <th>2011</th>\n",
              "      <th>2wmmip5</th>\n",
              "      <th>45</th>\n",
              "      <th>5wf9jg</th>\n",
              "      <th>610</th>\n",
              "      <th>ability</th>\n",
              "      <th>about</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc 1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        10   20  2010  2011  2wmmip5   45  5wf9jg  610  ability  about\n",
              "Doc 1  0.0  0.0   0.0   0.0      0.0  0.0     0.0  0.0      0.0    0.0\n",
              "Doc 2  0.0  0.0   0.0   0.0      0.0  0.0     0.0  0.0      0.0    0.0\n",
              "Doc 3  0.0  0.0   0.0   0.0      0.0  0.0     0.0  0.0      0.0    0.0\n",
              "Doc 4  0.0  0.0   0.0   0.0      0.0  0.0     0.0  0.0      0.0    0.0\n",
              "Doc 5  0.0  0.0   0.0   0.0      0.0  0.0     0.0  0.0      0.0    0.0"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_sample_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Entity</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>mentions</th>\n",
              "      <th>tokens</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im getting on borderlands and i will murder yo...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[im, getting, on, borderlands, and, i, will, m...</td>\n",
              "      <td>[im, get, on, borderland, and, i, will, murder...</td>\n",
              "      <td>im get on borderland and i will murder you all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>I am coming to the borders and I will kill you...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[I, am, coming, to, the, borders, and, I, will...</td>\n",
              "      <td>[I, be, come, to, the, border, and, I, will, k...</td>\n",
              "      <td>I be come to the border and I will kill you all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im getting on borderlands and i will kill you ...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[im, getting, on, borderlands, and, i, will, k...</td>\n",
              "      <td>[im, get, on, borderland, and, i, will, kill, ...</td>\n",
              "      <td>im get on borderland and i will kill you all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im coming on borderlands and i will murder you...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[im, coming, on, borderlands, and, i, will, mu...</td>\n",
              "      <td>[im, come, on, borderland, and, i, will, murde...</td>\n",
              "      <td>im come on borderland and i will murder you all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[im, getting, on, borderlands, 2, and, i, will...</td>\n",
              "      <td>[im, get, on, borderland, 2, and, i, will, mur...</td>\n",
              "      <td>im get on borderland 2 and i will murder you m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>2417</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Grounded almost looked pretty cool even despit...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[Grounded, almost, looked, pretty, cool, even,...</td>\n",
              "      <td>[Grounded, almost, look, pretty, cool, even, d...</td>\n",
              "      <td>Grounded almost look pretty cool even despite ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>2417</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Grounded looked cool despite the borderline un...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[Grounded, looked, cool, despite, the, borderl...</td>\n",
              "      <td>[Grounded, look, cool, despite, the, borderlin...</td>\n",
              "      <td>Grounded look cool despite the borderline unfu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>2417</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Grosskreutz looked pretty cool, even despite t...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[Grosskreutz, looked, pretty, cool, even, desp...</td>\n",
              "      <td>[Grosskreutz, look, pretty, cool, even, despit...</td>\n",
              "      <td>Grosskreutz look pretty cool even despite the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>2417</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Grounded almost looked pretty cool here despit...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[Grounded, almost, looked, pretty, cool, here,...</td>\n",
              "      <td>[Grounded, almost, look, pretty, cool, here, d...</td>\n",
              "      <td>Grounded almost look pretty cool here despite ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>2417</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Deep Grounded almost looked pretty cool even d...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[Deep, Grounded, almost, looked, pretty, cool,...</td>\n",
              "      <td>[Deep, Grounded, almost, looked, pretty, cool,...</td>\n",
              "      <td>Deep Grounded almost looked pretty cool even d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      ID       Entity Sentiment  \\\n",
              "0   2401  Borderlands  Positive   \n",
              "1   2401  Borderlands  Positive   \n",
              "2   2401  Borderlands  Positive   \n",
              "3   2401  Borderlands  Positive   \n",
              "4   2401  Borderlands  Positive   \n",
              "..   ...          ...       ...   \n",
              "95  2417  Borderlands  Negative   \n",
              "96  2417  Borderlands  Negative   \n",
              "97  2417  Borderlands  Negative   \n",
              "98  2417  Borderlands  Negative   \n",
              "99  2417  Borderlands  Negative   \n",
              "\n",
              "                                                Tweet hashtags mentions  \\\n",
              "0   im getting on borderlands and i will murder yo...                     \n",
              "1   I am coming to the borders and I will kill you...                     \n",
              "2   im getting on borderlands and i will kill you ...                     \n",
              "3   im coming on borderlands and i will murder you...                     \n",
              "4   im getting on borderlands 2 and i will murder ...                     \n",
              "..                                                ...      ...      ...   \n",
              "95  Grounded almost looked pretty cool even despit...                     \n",
              "96  Grounded looked cool despite the borderline un...                     \n",
              "97  Grosskreutz looked pretty cool, even despite t...                     \n",
              "98  Grounded almost looked pretty cool here despit...                     \n",
              "99  Deep Grounded almost looked pretty cool even d...                     \n",
              "\n",
              "                                               tokens  \\\n",
              "0   [im, getting, on, borderlands, and, i, will, m...   \n",
              "1   [I, am, coming, to, the, borders, and, I, will...   \n",
              "2   [im, getting, on, borderlands, and, i, will, k...   \n",
              "3   [im, coming, on, borderlands, and, i, will, mu...   \n",
              "4   [im, getting, on, borderlands, 2, and, i, will...   \n",
              "..                                                ...   \n",
              "95  [Grounded, almost, looked, pretty, cool, even,...   \n",
              "96  [Grounded, looked, cool, despite, the, borderl...   \n",
              "97  [Grosskreutz, looked, pretty, cool, even, desp...   \n",
              "98  [Grounded, almost, looked, pretty, cool, here,...   \n",
              "99  [Deep, Grounded, almost, looked, pretty, cool,...   \n",
              "\n",
              "                                               lemmas  \\\n",
              "0   [im, get, on, borderland, and, i, will, murder...   \n",
              "1   [I, be, come, to, the, border, and, I, will, k...   \n",
              "2   [im, get, on, borderland, and, i, will, kill, ...   \n",
              "3   [im, come, on, borderland, and, i, will, murde...   \n",
              "4   [im, get, on, borderland, 2, and, i, will, mur...   \n",
              "..                                                ...   \n",
              "95  [Grounded, almost, look, pretty, cool, even, d...   \n",
              "96  [Grounded, look, cool, despite, the, borderlin...   \n",
              "97  [Grosskreutz, look, pretty, cool, even, despit...   \n",
              "98  [Grounded, almost, look, pretty, cool, here, d...   \n",
              "99  [Deep, Grounded, almost, looked, pretty, cool,...   \n",
              "\n",
              "                                       processed_text  \n",
              "0      im get on borderland and i will murder you all  \n",
              "1     I be come to the border and I will kill you all  \n",
              "2        im get on borderland and i will kill you all  \n",
              "3     im come on borderland and i will murder you all  \n",
              "4   im get on borderland 2 and i will murder you m...  \n",
              "..                                                ...  \n",
              "95  Grounded almost look pretty cool even despite ...  \n",
              "96  Grounded look cool despite the borderline unfu...  \n",
              "97  Grosskreutz look pretty cool even despite the ...  \n",
              "98  Grounded almost look pretty cool here despite ...  \n",
              "99  Deep Grounded almost looked pretty cool even d...  \n",
              "\n",
              "[100 rows x 9 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "collapsed": true,
        "id": "BEEOcQHnITY6",
        "outputId": "f9911e3f-bd7b-4ad1-f57b-09b8a8882ae6"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'im getting on borderlands and i will murder you all ,'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m features = [\u001b[33m'\u001b[39m\u001b[33mTweet\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhashtags\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmentions\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlemmas\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mprocessed_text\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m corr_matrix = \u001b[43mprocessed_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m10\u001b[39m))\n\u001b[32m      9\u001b[39m sns.heatmap(corr_matrix,\n\u001b[32m     10\u001b[39m             annot=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     11\u001b[39m             fmt=\u001b[33m\"\u001b[39m\u001b[33m.2f\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m             linewidths=\u001b[32m.5\u001b[39m,\n\u001b[32m     16\u001b[39m             linecolor=\u001b[33m'\u001b[39m\u001b[33mblack\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/AI & ML Course/Graduation Project/Team-Route/.venv/lib/python3.13/site-packages/pandas/core/frame.py:11076\u001b[39m, in \u001b[36mDataFrame.corr\u001b[39m\u001b[34m(self, method, min_periods, numeric_only)\u001b[39m\n\u001b[32m  11074\u001b[39m cols = data.columns\n\u001b[32m  11075\u001b[39m idx = cols.copy()\n\u001b[32m> \u001b[39m\u001b[32m11076\u001b[39m mat = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m  11078\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mpearson\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m  11079\u001b[39m     correl = libalgos.nancorr(mat, minp=min_periods)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/AI & ML Course/Graduation Project/Team-Route/.venv/lib/python3.13/site-packages/pandas/core/frame.py:2002\u001b[39m, in \u001b[36mDataFrame.to_numpy\u001b[39m\u001b[34m(self, dtype, copy, na_value)\u001b[39m\n\u001b[32m   2000\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2001\u001b[39m     dtype = np.dtype(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m2002\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2003\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[32m   2004\u001b[39m     result = np.asarray(result, dtype=dtype)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/AI & ML Course/Graduation Project/Team-Route/.venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:1713\u001b[39m, in \u001b[36mBlockManager.as_array\u001b[39m\u001b[34m(self, dtype, copy, na_value)\u001b[39m\n\u001b[32m   1711\u001b[39m         arr.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1712\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1713\u001b[39m     arr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1714\u001b[39m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[32m   1715\u001b[39m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[32m   1717\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Study/AI & ML Course/Graduation Project/Team-Route/.venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:1772\u001b[39m, in \u001b[36mBlockManager._interleave\u001b[39m\u001b[34m(self, dtype, na_value)\u001b[39m\n\u001b[32m   1770\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1771\u001b[39m         arr = blk.get_values(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1772\u001b[39m     \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m = arr\n\u001b[32m   1773\u001b[39m     itemmask[rl.indexer] = \u001b[32m1\u001b[39m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask.all():\n",
            "\u001b[31mValueError\u001b[39m: could not convert string to float: 'im getting on borderlands and i will murder you all ,'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "features = ['Tweet', 'hashtags', 'mentions', 'tokens', 'lemmas','processed_text']\n",
        "corr_matrix = processed_df[features].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix,\n",
        "            annot=True,\n",
        "            fmt=\".2f\",\n",
        "            cmap='coolwarm',\n",
        "            cbar=True,\n",
        "            square=True,\n",
        "            linewidths=.5,\n",
        "            linecolor='black')\n",
        "plt.title('Correlation Matrix of Tweets Features and Sentiment')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCorrelation with Sentiment:\")\n",
        "print(corr_matrix['Sentiment'].sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60bbbe1e",
        "outputId": "469b456e-3414-47af-c28d-f6f37a081aa3"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = '/content/cardio_train.csv.zip'\n",
        "\n",
        "extraction_dir = '/content/'\n",
        "\n",
        "os.makedirs(extraction_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_dir)\n",
        "\n",
        "print(f\"'{zip_file_path}' unzipped to '{extraction_dir}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dd85bf87",
        "outputId": "455a70a3-58ae-414b-d998-e4239c686cd3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "features = ['age', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc',\n",
        "            'smoke', 'alco', 'active', 'gender', 'cardio']\n",
        "\n",
        "# Ensure all specified features exist in the DataFrame\n",
        "missing_features = [f for f in features if f not in processed_df.columns]\n",
        "if missing_features:\n",
        "    print(f\"Warning: The following features are missing from processed_df: {missing_features}\")\n",
        "    # Filter out missing features for correlation calculation\n",
        "    available_features = [f for f in features if f in processed_df.columns]\n",
        "else:\n",
        "    available_features = features\n",
        "\n",
        "corr_matrix = processed_df[available_features].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix,\n",
        "            annot=True,\n",
        "            fmt=\".2f\",\n",
        "            cmap='coolwarm',\n",
        "            cbar=True,\n",
        "            square=True,\n",
        "            linewidths=.5,\n",
        "            linecolor='black')\n",
        "plt.title('Correlation Matrix of Clinical Features and Cardiovascular Disease')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCorrelation with Cardiovascular Disease (cardio):\")\n",
        "if 'cardio' in available_features:\n",
        "    print(corr_matrix['cardio'].sort_values(ascending=False))\n",
        "else:\n",
        "    print(\" 'cardio' feature is not available for correlation calculation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "XeQEXf6JITo-",
        "outputId": "e6e9b23d-1aaf-4fc0-c137-18c419734047"
      },
      "outputs": [],
      "source": [
        "df_eda = processed_df.copy()\n",
        "df_eda['age_years'] = (df_eda['age'] / 365.25).round().astype(int)\n",
        "\n",
        "min_age = df_eda['age_years'].min()\n",
        "max_age = df_eda['age_years'].max()\n",
        "bins = range(min_age, max_age + 6, 5)\n",
        "\n",
        "df_eda['age_bin'] = pd.cut(df_eda['age_years'],\n",
        "                           bins=bins,\n",
        "                           right=False,\n",
        "                           labels=[f'{i}-{i+4}' for i in bins[:-1]])\n",
        "\n",
        "prevalence_by_age = df_eda.groupby('age_bin', observed=False)['cardio'].agg(\n",
        "    total_patients=('size'),\n",
        "    disease_prevalence=('mean')\n",
        ").reset_index()\n",
        "\n",
        "print(\"Disease Prevalence by 5-Year Age Bin:\")\n",
        "print(prevalence_by_age)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='age_bin', y='disease_prevalence', data=prevalence_by_age, palette='viridis')\n",
        "plt.title('Cardiovascular Disease Prevalence by 5-Year Age Group')\n",
        "plt.xlabel('Age Bin (Years)')\n",
        "plt.ylabel('Disease Prevalence (Ratio)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "GDG8HBsCIT4X",
        "outputId": "489e4f35-ad4d-4b81-b7bd-0a5e2dd4d97b"
      },
      "outputs": [],
      "source": [
        "def categorize_bp(ap_hi, ap_lo):\n",
        "    if ap_hi < 120 and ap_lo < 80:\n",
        "        return 'Normal'\n",
        "    elif (ap_hi >= 140 or ap_lo >= 90):\n",
        "        return 'Stage 2 Hypertension'\n",
        "    elif (ap_hi >= 130 and ap_hi < 140) or (ap_lo >= 80 and ap_lo < 90):\n",
        "        return 'Stage 1 Hypertension'\n",
        "    elif (ap_hi >= 120 and ap_hi < 130) and (ap_lo < 80):\n",
        "        return 'Elevated/Pre-Hypertension'\n",
        "    else:\n",
        "        return 'Stage 1 Hypertension'\n",
        "\n",
        "df_eda['bp_category'] = df_eda.apply(\n",
        "    lambda row: categorize_bp(row['ap_hi'], row['ap_lo']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "bp_counts = df_eda['bp_category'].value_counts().sort_index()\n",
        "sns.barplot(x=bp_counts.index, y=bp_counts.values, palette='plasma')\n",
        "plt.title('Patient Distribution by Blood Pressure Category')\n",
        "plt.xlabel('Blood Pressure Category')\n",
        "plt.ylabel('Number of Patients')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Nmk94hV8IUFK",
        "outputId": "ac991a2a-2601-4921-e20e-7ba42b701086"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(df_eda,\n",
        "                 x='age_years',\n",
        "                 y='weight',\n",
        "                 color='cardio',\n",
        "                 color_discrete_map={0: 'blue', 1: 'red'},\n",
        "                 title='Age vs. Weight by Cardiovascular Disease Status',\n",
        "                 labels={'cardio': 'Cardio Disease (1=Yes, 0=No)'},\n",
        "                 hover_data=['ap_hi', 'ap_lo', 'cholesterol'])\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6a4fD6AIUU7",
        "outputId": "fb5f8e40-3215-4080-a04b-c93b4af3088f"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "contingency_table = pd.crosstab(df_eda['cholesterol'], df_eda['cardio'])\n",
        "\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"Chi-Square Test: Cholesterol vs. Cardiovascular Disease\")\n",
        "print(f\"Chi-square statistic: {chi2:.2f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Conclusion: The relationship between cholesterol level and cardiovascular disease is statistically significant.\")\n",
        "else:\n",
        "    print(\"Conclusion: The relationship is NOT statistically significant.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
